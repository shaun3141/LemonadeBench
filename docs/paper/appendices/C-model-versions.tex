\section{Model Versions}
\label{app:versions}

All experiments were conducted in December 2025 using OpenRouter as a unified API gateway to access models from multiple providers. This approach ensures consistent API behavior and enables reproducible comparisons across providers.

\subsection{Model Selection Criteria}

We selected 20 models across five tiers based on:
\begin{enumerate}
    \item \textbf{Agentic capability}: Demonstrated tool/function calling support
    \item \textbf{Reasoning benchmarks}: Performance on MATH-500, ARC-AGI, SWE-Bench
    \item \textbf{Cost diversity}: Range from \$0.075/M to \$75/M tokens
    \item \textbf{Accessibility}: Including open-source options for reproducibility
    \item \textbf{Provider diversity}: Representation from 7 providers
\end{enumerate}

\subsection{Evaluated Models}

\begin{table}[h]
\centering
\caption{Complete Model Specifications}
\label{tab:versions}
\small
\begin{tabular}{@{}llrrl@{}}
\toprule
\textbf{Model} & \textbf{Provider} & \textbf{Input/M} & \textbf{Output/M} & \textbf{Tier} \\
\midrule
\multicolumn{5}{l}{\emph{Tier 1: Premium Reasoning}} \\
claude-sonnet-4 & Anthropic & \$3.00 & \$15.00 & Premium \\
claude-opus-4.5 & Anthropic & \$15.00 & \$75.00 & Premium \\
o1 & OpenAI & \$15.00 & \$60.00 & Premium \\
gpt-5.1 & OpenAI & \$5.00 & \$15.00 & Premium \\
gemini-3-pro & Google & \$2.50 & \$15.00 & Premium \\
\midrule
\multicolumn{5}{l}{\emph{Tier 2: Balanced Performance}} \\
claude-3.5-sonnet & Anthropic & \$3.00 & \$15.00 & Balanced \\
o3-mini & OpenAI & \$1.10 & \$4.40 & Balanced \\
gemini-2.5-flash & Google & \$0.30 & \$2.50 & Balanced \\
grok-2 & X.AI & \$5.00 & \$10.00 & Balanced \\
\midrule
\multicolumn{5}{l}{\emph{Tier 3: Cost-Effective}} \\
deepseek-r1 & DeepSeek & \$0.55 & \$2.19 & Value \\
deepseek-v3 & DeepSeek & \$0.27 & \$1.10 & Value \\
qwq-32b & Qwen & \$0.20 & \$0.20 & Value \\
mistral-small-3 & Mistral & \$0.10 & \$0.30 & Value \\
\midrule
\multicolumn{5}{l}{\emph{Tier 4: Open Source}} \\
llama-3.3-70b-instruct & Meta & \$0.40 & \$0.40 & Open \\
llama-3.1-405b-instruct & Meta & \$2.00 & \$2.00 & Open \\
qwen-2.5-72b-instruct & Qwen & \$0.35 & \$0.40 & Open \\
mistral-large & Mistral & \$2.00 & \$6.00 & Open \\
\midrule
\multicolumn{5}{l}{\emph{Tier 5: Fast \& Efficient}} \\
claude-3.5-haiku & Anthropic & \$0.80 & \$4.00 & Fast \\
gpt-4o-mini & OpenAI & \$0.15 & \$0.60 & Fast \\
gemini-flash-1.5 & Google & \$0.075 & \$0.30 & Fast \\
\bottomrule
\end{tabular}
\begin{flushleft}
\small Prices in USD per million tokens via OpenRouter (December 2025).
\end{flushleft}
\end{table}

\subsection{Model Capabilities}

Table~\ref{tab:model-capabilities} summarizes key capabilities relevant to the LemonadeBench task.

\begin{table}[h]
\centering
\caption{Model Capabilities for Agentic Tasks}
\label{tab:model-capabilities}
\small
\begin{tabular}{@{}lcccr@{}}
\toprule
\textbf{Model} & \textbf{Tool Calling} & \textbf{Context} & \textbf{Reasoning} & \textbf{MATH-500} \\
\midrule
claude-opus-4.5 & \checkmark & 200K & Extended & --- \\
claude-sonnet-4 & \checkmark & 200K & Standard & --- \\
o1 & \checkmark & 200K & CoT & 94.8\% \\
o3-mini & \checkmark & 200K & CoT & --- \\
gpt-5.1 & \checkmark & 200K & Extended & --- \\
gemini-3-pro & \checkmark & 2M & Extended & --- \\
deepseek-r1 & \checkmark & 64K & CoT & 97.3\% \\
deepseek-v3 & \checkmark & 128K & Standard & 92.5\% \\
qwq-32b & \checkmark & 32K & CoT & --- \\
llama-3.3-70b & \checkmark & 128K & Standard & --- \\
llama-3.1-405b & \checkmark & 128K & Standard & --- \\
qwen-2.5-72b & \checkmark & 128K & Standard & --- \\
\bottomrule
\end{tabular}
\begin{flushleft}
\small CoT = Chain-of-Thought reasoning. MATH-500 scores from model release papers where available.
\end{flushleft}
\end{table}

\subsection{API Configuration}

All models were accessed via OpenRouter's unified API with the following settings:
\begin{itemize}
    \item \textbf{Base URL}: \texttt{https://openrouter.ai/api/v1}
    \item \textbf{Max tokens}: 1024 per response
    \item \textbf{Temperature}: 0 (greedy decoding)
    \item \textbf{Top-p}: 1.0
    \item \textbf{Tool choice}: Required (forced function calling)
\end{itemize}

Greedy decoding ($T=0$) ensures full reproducibility and isolates the effects of goal-framing prompts from sampling stochasticity. Given the same environment seed and goal-framing condition, a model produces identical behavior across runs. Environmental variance across the 5--10 random seeds provides sufficient stochastic variation for statistical analysis, making sampling variance unnecessary and scientifically undesirable.

\subsection{Cost Analysis}

Total evaluation cost across 1,000 episodes reflects three experimental components:

\begin{table}[h]
\centering
\caption{Estimated Evaluation Cost by Experiment Type}
\label{tab:cost}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Tier} & \textbf{Goal-Framing} & \textbf{Ablations} & \textbf{Total Est. Cost} \\
 & \textbf{(30 eps)} & \textbf{(80 eps)} & \\
\midrule
Premium (5 models) & \$36--60 & \$96--160 & \$132--220 \\
Balanced (4 models) & \$24--48 & --- & \$24--48 \\
Value (4 models) & \$6--12 & \$6--12 & \$12--24 \\
Open Source (4 models) & \$12--24 & \$12--24 & \$24--48 \\
Fast (3 models) & \$3--6 & --- & \$3--6 \\
\midrule
\multicolumn{3}{r}{\textbf{Grand Total (1,000 episodes):}} & \textbf{\$195--346} \\
\bottomrule
\end{tabular}
\begin{flushleft}
\small Estimates based on ~2,000 input tokens and ~500 output tokens per turn, 14 turns per episode. Ablations include 5 flagship models only (see Table~\ref{tab:model-categories}). Architecture ablations with \textsc{Full} loop require 3$\times$ the base cost due to planning and reflection phases.
\end{flushleft}
\end{table}

\paragraph{Cost-Effectiveness.}
At the reported price ranges, evaluating all 20 models across goal-framing conditions costs approximately \$81--150 (600 episodes). The most cost-effective tier for comprehensive evaluation is Value (DeepSeek R1, DeepSeek V3, QwQ-32B), providing strong reasoning at <\$0.50/episode.

\subsection{Reproducibility}

To reproduce our experiments:

\begin{enumerate}
    \item Obtain an OpenRouter API key at \texttt{https://openrouter.ai/keys}
    \item Set environment variable: \texttt{export OPENROUTER\_API\_KEY="your-key"}
    \item Run the batch configuration:
    \begin{verbatim}
    lemonade batch examples/batch_config.yaml --parallel 4
    \end{verbatim}
\end{enumerate}

The exact configuration file is included in the supplementary materials and available at the project repository.
