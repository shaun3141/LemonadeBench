\section{Results}
\label{sec:results}

We now present key findings organized by our pre-registered hypotheses.

\subsection{Hypothesis Testing}
\label{sec:hypothesis-results}

\begin{table}[h]
\centering
\caption{Pre-Registered Hypothesis Results}
\label{tab:hypotheses}
\small
\begin{tabular}{@{}cp{6.5cm}ccc@{}}
\toprule
\textbf{H\#} & \textbf{Hypothesis} & \textbf{Result} & \textbf{$p$-value} & \textbf{Effect} \\
\midrule
\multicolumn{5}{l}{\emph{Goal-Framing}} \\
H1 & \textsc{Competitive} increases risk-taking & \textcolor{gray}{TBD} & --- & $d$ = X.XX \\
H2 & \textsc{Conservative}/\textsc{Survival} reduces spoilage & \textcolor{gray}{TBD} & --- & $d$ = X.XX \\
H3 & \textsc{Aggressive} helps hot days, hurts bad days & \textcolor{gray}{TBD} & --- & $d$ = X.XX \\
H4 & \textsc{Growth} increases early exploration & \textcolor{gray}{TBD} & --- & $d$ = X.XX \\
H5 & Larger models more sensitive to framing & \textcolor{gray}{TBD} & --- & $r$ = X.XX \\
\midrule
\multicolumn{5}{l}{\emph{Architecture}} \\
H6 & \textsc{Plan-Act} reduces spoilage & \textcolor{gray}{TBD} & --- & $d$ = X.XX \\
H7 & \textsc{Act-Reflect} improves in late game & \textcolor{gray}{TBD} & --- & $d$ = X.XX \\
H8 & \textsc{Full} shows diminishing returns & \textcolor{gray}{TBD} & --- & $d$ = X.XX \\
H9 & Plans frequently abandoned mid-episode & \textcolor{gray}{TBD} & --- & X\% \\
\midrule
\multicolumn{5}{l}{\emph{Scaffolding}} \\
H10 & Calculator reduces pricing errors & \textcolor{gray}{TBD} & --- & $d$ = X.XX \\
H11 & Math prompts help even without tools & \textcolor{gray}{TBD} & --- & $d$ = X.XX \\
H12 & Code interpreter underutilized & \textcolor{gray}{TBD} & --- & X\% use \\
H13 & All LLMs underperform Reactive on spoilage & \textcolor{gray}{TBD} & --- & --- \\
\bottomrule
\end{tabular}
\begin{flushleft}
\small Effect sizes: Cohen's $d$ for continuous outcomes (small $\geq 0.2$, medium $\geq 0.5$, large $\geq 0.8$); Pearson's $r$ for correlations. All tests use Bonferroni correction within each hypothesis family.
\end{flushleft}
\end{table}

\subsection{Key Finding 1: [Title TBD]}
\label{sec:finding1}

% Placeholder for main finding
\emph{[Describe the most surprising or important finding here. Include statistical details and interpretation.]}

\subsection{Key Finding 2: [Title TBD]}
\label{sec:finding2}

% Placeholder for second finding
\emph{[Describe another key finding, potentially about model differences or a specific condition effect.]}

\subsection{Key Finding 3: [Title TBD]}
\label{sec:finding3}

% Placeholder for third finding
\emph{[Describe a finding about failure modes or qualitative behavioral patterns.]}

\subsection{Model Comparison}
\label{sec:model-comparison}

Figure~\ref{fig:model-radar} provides a multi-dimensional comparison of model capabilities.

\begin{figure}[h]
\centering
\fbox{\parbox{0.8\textwidth}{\centering\vspace{2cm}\textbf{[PLACEHOLDER]}\\ Radar/spider chart showing each model's performance on:\\ - Total profit (normalized)\\ - Spoilage rate (inverted)\\ - Weather adaptation\\ - Recovery score\\ - Prompt sensitivity\vspace{2cm}}}
\caption{Multi-dimensional model comparison. [Model X] shows the best overall profile; [Model Y] excels at [specific dimension] but struggles with [other dimension].}
\label{fig:model-radar}
\end{figure}

\subsection{Comparison with Reinforcement Learning Baseline}
\label{sec:rl-baseline}

To contextualize LLM agent performance, we trained a traditional reinforcement learning agent using Proximal Policy Optimization (PPO)~\citep{schulman2017proximal} via Stable Baselines3~\citep{stable-baselines3}. This provides a reference point for what a learning-based approach can achieve with full access to the environment dynamics.

\paragraph{Architecture and Training.}
The PPO agent uses a two-layer MLP policy with 64 hidden units per layer ($\sim$12K parameters), processing a 27-dimensional continuous observation encoding weather conditions, inventory levels, cash, reputation, and temporal features. Actions are decoded from an 8-dimensional continuous output space covering price setting and inventory purchases. The agent was trained for 4 million timesteps using:
\begin{itemize}[noitemsep]
    \item Generalized Advantage Estimation (GAE) with $\lambda = 0.95$
    \item Rollout buffer of 4,096 steps across 8 parallel environments
    \item Linearly decaying learning rate from $3 \times 10^{-4}$ to 0
    \item Observation normalization via running statistics
    \item \textbf{Randomized episode seeds} for generalization across weather patterns
\end{itemize}

Training completed in approximately 12 minutes on an AMD Ryzen 7 9800X3D CPU at $\sim$5,500 steps/second. Notably, the small MLP policy does not benefit from GPU acceleration---the environment simulation (running in Python) is the primary bottleneck.

\paragraph{Results.}
Table~\ref{tab:rl-baseline} compares the PPO agent against LLM agents on the same 10 evaluation seeds used throughout this paper.

\begin{table}[h]
\centering
\caption{PPO Baseline vs.\ LLM Agents on Paper Methodology Seeds}
\label{tab:rl-baseline}
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Agent} & \textbf{Mean Profit} & \textbf{Std Dev} & \textbf{Min} & \textbf{Max} & \textbf{Cups Sold} \\
\midrule
PPO Baseline (4M steps) & \$584.45 & \$98.31 & \$475.88 & \$778.87 & 573.6 \\
\midrule
\emph{[Best LLM Agent]} & \emph{[TBD]} & \emph{[TBD]} & \emph{[TBD]} & \emph{[TBD]} & \emph{[TBD]} \\
\emph{[Median LLM Agent]} & \emph{[TBD]} & \emph{[TBD]} & \emph{[TBD]} & \emph{[TBD]} & \emph{[TBD]} \\
Random Baseline & \emph{[TBD]} & \emph{[TBD]} & \emph{[TBD]} & \emph{[TBD]} & \emph{[TBD]} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Interpretation.}
The PPO agent achieves strong performance (\$584 mean profit) through millions of trial-and-error interactions with the environment, learning implicit patterns about weather-dependent pricing and inventory management. However, this comes with important caveats:

\begin{enumerate}[noitemsep]
    \item \textbf{No interpretability}: Unlike LLM agents that produce natural language reasoning, PPO's policy is a black-box neural network with no explanatory capacity.
    
    \item \textbf{No transfer}: The PPO agent is trained specifically on this environment; LLM agents apply general reasoning that could transfer to similar domains.
    
    \item \textbf{Sample efficiency}: PPO required 4M environment interactions ($\sim$285K episodes). LLM agents operate zero-shot or with minimal in-context examples.
    
    \item \textbf{High variance}: The PPO agent shows $\pm$\$98 standard deviation across seeds, reflecting genuine difficulty variance in the environment rather than reasoning inconsistency.
\end{enumerate}

The PPO baseline establishes that LemonadeBench is learnable through gradient-based optimization, while highlighting the distinct value proposition of LLM agents: interpretable reasoning, transfer potential, and sample efficiency---at the cost of [higher/lower/comparable] absolute performance.

