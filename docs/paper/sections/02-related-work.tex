\section{Related Work}
\label{sec:related}

\paragraph{LLM Agent Benchmarks.}
The evaluation of LLM-based agents has expanded rapidly in 2024--2025. \textbf{AgentBench} \citep{liu2023agentbench} provides a multi-domain framework assessing planning, tool use, and self-reflection across eight environments. \textbf{SWE-bench} \citep{jimenez2024swebench} evaluates software engineering capabilities by tasking agents with resolving real GitHub issues; state-of-the-art agents achieve approximately 20\% success rates on the full benchmark and 43\% on the human-verified SWE-bench Lite subset as of late 2024. \textbf{WebArena} \citep{zhou2024webarena} and its successor \textbf{WebChoreArena} test web navigation through realistic browser interactions, with tasks requiring multi-page memory and complex calculations. \textbf{AgentGym} \citep{xi2024agentgym} offers a modular framework spanning 14 environments for training agents across diverse tasks with standardized HTTP interfaces.

These benchmarks share a common structure: each task is an independent episode with binary success/failure outcomes. LemonadeBench differs by requiring \emph{sequential} decision-making where performance compounds across 14 days---early mistakes constrain late-game options, and recovery strategies matter.

\paragraph{Economic and Business Simulation.}
Recent work has explored LLMs in economic contexts. \textbf{Homo Silicus} \citep{horton2023llmeconomicagents} demonstrates that LLMs can serve as computational models of human economic behavior, replicating classic behavioral economics experiments. \textbf{PartnerMAS} \citep{li2025partnermas} uses multi-agent hierarchies for business partner selection in venture capital contexts. The \textbf{LLM Economist} \citep{karten2025llmeconomist} framework applies agent-based modeling to tax policy design using demographically realistic populations. However, these focus on \emph{isolated decisions} or \emph{simulating human preferences} rather than operating a business over time.

\paragraph{Multi-Day Business Benchmarks.}
Most relevant to our work are recent multi-day business simulations designed specifically for LLM evaluation. \textbf{Vending-Bench} \citep{backlund2025vendingbench} tasks agents with managing a vending machine over multiple days, making decisions about inventory, pricing, and fees---demonstrating that long-horizon coherence is a meaningful challenge for LLM agents. \citet{ovezmyradov2025aiplayingbusiness} introduce an open-access retail management simulator running a 12-month decision-making game with pricing, orders, and marketing decisions. These works establish that multi-day business simulation is a valuable testbed for LLM capabilities.

LemonadeBench builds on this emerging research direction while contributing three distinct elements: (1) \textbf{richer game mechanics} including location-based risk/reward tradeoffs with different weather exposures, perishable inventory with FIFO spoilage tracking, and a reputation system creating delayed feedback; (2) \textbf{systematic study of LLM-specific factors} including goal-framing prompts, agent architectures, and cognitive scaffolding---aspects not examined in prior business benchmarks; and (3) \textbf{interpretability} through a domain (lemonade stand) where every action maps to intuitions humans universally understand, enabling qualitative analysis of agent reasoning.

\paragraph{Long-Horizon Planning.}
Sequential decision-making benchmarks have emerged to test planning capabilities. \textbf{PlanningArena} \citep{zheng2025planningarena} provides modular evaluation of planning dimensions including step execution and dependency accuracy, with GPT-4o achieving 56.5\% and DeepSeekV3 achieving 41.9\% on the benchmark. Other work has explored long-horizon sequential tasks in game environments, though these typically lack real-world interpretability.

LemonadeBench occupies a middle ground: episodes are short enough for tractable evaluation (14 steps) yet long enough for meaningful strategy emergence. Unlike abstract games, every action has real-world interpretability (``buy lemons,'' ``raise price''), enabling qualitative analysis of failures.

\paragraph{Reinforcement Learning Environments.}
The RL community has developed extensive environment suites, from OpenAI Gym to more recent frameworks for specific domains. These focus primarily on \emph{training} RL agents through trial-and-error learning, whereas LemonadeBench is designed for \emph{evaluating} pre-trained LLMs without fine-tuning---testing whether models can apply general reasoning to novel domains based solely on instructions and feedback.

\paragraph{Positioning LemonadeBench.}
Table~\ref{tab:comparison} summarizes how LemonadeBench relates to existing benchmarks. A key distinction is that LemonadeBench systematically examines prompt engineering effects (goal-framing) on agent behavior, whereas other benchmarks focus solely on task success rates.

\begin{table}[h]
\centering
\caption{Comparison with Related Benchmarks}
\label{tab:comparison}
\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Benchmark} & \textbf{Domain} & \textbf{Horizon} & \textbf{Stochastic} & \textbf{Interpretable} & \textbf{Compounding} & \textbf{Prompt Test} \\
\midrule
SWE-bench & Code & Single & No & Medium & No & No \\
WebArena & Web & Single & No & Low & No & No \\
AgentBench & Multi & Single & Partial & Low & No & No \\
Homo Silicus & Economics & Single & No & High & No & No \\
PlanningArena & Planning & Multi & Yes & Medium & Partial & No \\
\midrule
Vending-Bench & Business & Multi & Yes & Medium & Yes & No \\
Retail Sim & Business & Multi (12mo) & Yes & Medium & Yes & No \\
\midrule
\textbf{LemonadeBench} & Business & Multi (14d) & Yes & High & Yes & \textbf{Yes} \\
\bottomrule
\end{tabular}
\end{table}

The key differentiator for LemonadeBench is not the multi-day business format itself---which Vending-Bench and retail simulators have established as valuable---but rather the \emph{systematic investigation of LLM-specific factors}: how goal framing affects economic behavior, which agent architectures perform best over long horizons, and whether cognitive scaffolding (calculators, code interpreters) improves reasoning. These studies address gaps in prior work that focused primarily on task completion rates rather than understanding the factors that modulate LLM agent behavior.
